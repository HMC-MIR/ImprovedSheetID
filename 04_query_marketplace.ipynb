{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Marketplace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook constructs a \"query marketplace\", where our system selects the fingerprints with the highest utility to cost ratio at runtime and does lookups only on those fingerprints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from ExtractBootlegScores import *\n",
    "import itertools\n",
    "import numba as nb\n",
    "from numba import njit\n",
    "from collections import defaultdict\n",
    "import dill\n",
    "from glob import iglob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load in databases and counts**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this dictionary contains all the databases for each n-gram type included in our marketplace database\n",
    "db_dir = \"/data1/kji/databases_v4d/105mill\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this dictionary maps each n-gram type to its probability of correctness\n",
    "probabilities_dir = \"/data1/kji/databases_random/probabilities.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this dictionary maps each number to the piece name (a string)\n",
    "piece_mapping_dir = \"num_to_piece.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in iglob(f\"{db_dir}/*.pkl\", recursive=True):\n",
    "    combination = filename.split('/')[-1][:-4]\n",
    "    with open(filename, \"rb\") as f:\n",
    "        d[combination] = pickle.load(f)\n",
    "    print(f\"finished {combination}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(probabilities_dir, \"rb\") as f:\n",
    "    utilities = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(piece_mapping_dir, 'rb') as f:\n",
    "    num_to_piece = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combinations = []\n",
    "for n_gram in range(1, 4):\n",
    "    combinations += [[0] + list(tup) for tup in itertools.combinations(range(1, 7), n_gram-1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combinations = [\"\".join(str(num) for num in combination) for combination in combinations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "powers = 1 << np.arange(62)[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_fingerprint(cols):\n",
    "    fp = []\n",
    "    equals_Zero = True\n",
    "    for column in cols:\n",
    "        hashint = int(column.dot(powers))\n",
    "        fp.append(hashint)\n",
    "        if hashint != 0:\n",
    "            equals_Zero = False\n",
    "    if equals_Zero == True:\n",
    "        return None\n",
    "    return tuple(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our utility to cost ratio is defined as the probability of correctness (utility) divided by the number of matches (cost), because the number of matches equals the number of lookups we would have to do for that fingerprint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def utility(combination, matches):\n",
    "    return utilities[combination] / matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ratios(bscore_query, rindex_dict):\n",
    "    \"\"\"Inputs: an L x 62 bootleg score query and our dictionary, where\n",
    "               rindex_dict[fp] = (count, {dictionary of pieces and offsets})\n",
    "        Output: a 16 X L table where each element is a tuple of (utility:cost ratio, combination, n_gram)\"\"\"\n",
    "    l = len(bscore_query)\n",
    "    # ratios[i][j] is a pair of (ratio, combination, fingerprint)\n",
    "    ratios = np.array([[(0, None, None) for _ in range(l)] for _ in range(16)])\n",
    "    for j in range(l):\n",
    "        # calculate utility to cost ratio for all 16 n-grams\n",
    "        for idx, combination in enumerate(combinations):\n",
    "            cols = []\n",
    "            # we need at least enough fingerprints for all the indices in our combination\n",
    "            try:\n",
    "                for i in combination:\n",
    "                    cols.append(bscore_query[j+int(i)])\n",
    "            except IndexError:\n",
    "                continue\n",
    "            fp = compute_fingerprint(cols)\n",
    "            if not fp or combination not in rindex_dict or fp not in rindex_dict[combination]:\n",
    "                continue\n",
    "            matches = rindex_dict[combination][fp][0]\n",
    "            if matches == 0:\n",
    "                continue\n",
    "            ratios[idx][j] = (utility(combination, matches), combination, fp)\n",
    "    return ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_offset_dict(offset_dict, pieces_and_offsets, i, num_lookups):\n",
    "    \"\"\"Input: a dictionary mapping pieces to offsets for a given n-gram and the number of lookups we can do.\n",
    "       Output: an updated dictionary of offsets containing all the lookups we just did. This dictionary\n",
    "               will be used in the histogram of offsets.\"\"\"\n",
    "    if num_lookups == 0:\n",
    "        return\n",
    "    for piece in pieces_and_offsets:\n",
    "        if num_lookups <= len(pieces_and_offsets[piece]):\n",
    "            offset = [j - i for j in pieces_and_offsets[piece][:num_lookups]]\n",
    "        else:\n",
    "            offset = [j - i for j in pieces_and_offsets[piece]]\n",
    "        offset_dict[num_to_piece[piece]].extend(offset)\n",
    "        num_lookups -= len(pieces_and_offsets[piece])\n",
    "        if num_lookups <= 0:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fingerprints(bscore_query, rindex_dict, ratios, runtime_budget):\n",
    "    \"\"\"This takes in a bootleg score for a query as its input, and does a certain number of lookups for each\n",
    "       column of the bootleg score in accordance with our runtime budget. It then returns the updated dictionary of\n",
    "       offsets containing all the lookups performed for the query, which is then used to compute the histogram of \n",
    "       offsets.\"\"\"\n",
    "    l = len(bscore_query)\n",
    "    aisle_budget = runtime_budget // l\n",
    "    cur_budget = aisle_budget\n",
    "    offset_dict = defaultdict(list)\n",
    "    matches_processed = 0\n",
    "    for i in range(l):\n",
    "        fingerprints = []\n",
    "        col = ratios[:, i]\n",
    "        lookups = sorted(col, key = lambda x: x[0], reverse = True)\n",
    "        for _, combination, n_gram in lookups:\n",
    "            if not n_gram or cur_budget < 0:\n",
    "                break\n",
    "            matches, pieces_and_offsets = rindex_dict[combination][n_gram]\n",
    "            if cur_budget - matches < 0:\n",
    "                num_lookups = cur_budget\n",
    "            else:\n",
    "                num_lookups = matches\n",
    "            update_offset_dict(offset_dict, pieces_and_offsets, i, num_lookups)\n",
    "            cur_budget -= num_lookups\n",
    "            matches_processed += num_lookups\n",
    "        cur_budget += aisle_budget\n",
    "    return offset_dict, matches_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rankHistograms(offset_dict, bin_size=10):\n",
    "    \"\"\"This implements the histogram of offsets method for ranking the predicted pieces.\"\"\"\n",
    "    bin_size = 3\n",
    "    pieceScores = []\n",
    "    for key, h in offset_dict.items():\n",
    "        if not h:\n",
    "            continue\n",
    "        maxh = max(h)\n",
    "        minh = min(h)\n",
    "        hist = np.zeros(int((maxh-minh)/bin_size)+2)\n",
    "        for i in h:\n",
    "            hist[int((i-minh)/bin_size)] += 1\n",
    "        score = np.max(hist)\n",
    "        pieceScores.append((key, score))\n",
    "            \n",
    "    pieceScores = sorted(pieceScores, key = lambda x:x[1], reverse=True)\n",
    "    return pieceScores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processSingleQuery(imagefile, rindex, runtime_budget, outfile = None):\n",
    "    \"\"\"Inputs: a file representing a query image, a reverse index dictionary mapping each n-gram to its \n",
    "               offsets in IMSLP, and a runtime budget for the query.\n",
    "       Output: a sorted list of predicted pieces and their scores based on the histogram of offsets method.\"\"\"\n",
    "    profileStart = time.time()\n",
    "    \n",
    "    # Get Bootleg Score\n",
    "    bscore_query = processQuery(imagefile)\n",
    "    bscore_query = bscore_query.T\n",
    "    \n",
    "    searchStart = time.time()\n",
    "    # Generate and rank histograms\n",
    "    \n",
    "    ratios = get_ratios(bscore_query, rindex)\n",
    "    offset_dict, matches_processed = get_fingerprints(bscore_query, rindex, ratios, runtime_budget)\n",
    "    pieceScores = rankHistograms(offset_dict)\n",
    "    # Profile & save to file\n",
    "    profileEnd = time.time()\n",
    "    \n",
    "    profileDur = profileEnd - profileStart\n",
    "    print(matches_processed)\n",
    "    print(profileDur)\n",
    "    saveToFile(outfile, imagefile, pieceScores, profileDur, matches_processed)\n",
    "    return pieceScores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processSingleQuery('data/queries/p146_q2.jpg', d, 75000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveToFile(outfile, imagefile, pieceScores, profileDur, matches_processed):\n",
    "    if outfile:\n",
    "        with open(outfile, 'wb') as f:\n",
    "            query = os.path.splitext(os.path.basename(imagefile))[0]\n",
    "            pickle.dump((query,pieceScores, profileDur, matches_processed),f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processQuery_wrapper(queryfile, rindex, outdir, runtime_budget):\n",
    "    # wrapper for running multiple jobs in parallel\n",
    "    basename = os.path.splitext(os.path.basename(queryfile))[0] # e.g. p1_q1\n",
    "    hyp_outfile = \"{}/{}.hyp\".format(outdir, basename)\n",
    "    piece = basename.split('_')[0]\n",
    "    # might change later to print to outfile\n",
    "    return processSingleQuery(queryfile, rindex, runtime_budget, hyp_outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we set the runtime budget to 65000, which means we can process at most 65000 matches per query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runtime_budget = 65000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "query_list = 'cfg_files/query.test.list' # list of query images\n",
    "outdir = f'experiments/v0.4.0d_test_65k_budget/hyp' # where to save hypothesis output files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prep output directory\n",
    "if not os.path.isdir(outdir):\n",
    "    os.makedirs(outdir)\n",
    "\n",
    "# load reverse index. Recommend keeping load=False and loading it earlier.\n",
    "load = False\n",
    "if load:\n",
    "    print(\"LOADING RINDEX\")\n",
    "    rindex1 = []\n",
    "    with open(pickle_file, 'rb') as f:\n",
    "        rindex1 = pickle.load(f)\n",
    "    rindex_filter = rindex1\n",
    "\n",
    "print(\"STARTING PROCESSING\")\n",
    "# number of cores to use\n",
    "multiprocess = False\n",
    "if multiprocess:\n",
    "    n_cores = 30 #multiprocessing.cpu_count()\n",
    "    pool = multiprocessing.Pool(processes=n_cores)\n",
    "\n",
    "inputs = []\n",
    "with open(query_list, 'r') as f:\n",
    "    for line in f:\n",
    "        inputs.append((line.rstrip(), outdir))\n",
    "\n",
    "if multiprocess:\n",
    "    # process queries in parallel\n",
    "    outputs = list(pool.starmap(processQuery_wrapper, inputs))\n",
    "else:\n",
    "    for i in inputs:\n",
    "        processQuery_wrapper(i[0], d, i[1], runtime_budget)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
