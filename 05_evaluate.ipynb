{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the system performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook evaluates the performance of the system by comparing the ranked list of pieces with the ground truth for each query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import os.path\n",
    "import pickle\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readGroundTruthLabels(gtfile):\n",
    "    d={}\n",
    "    d1={}\n",
    "    with open(gtfile, \"r\") as f:\n",
    "        for line in f:\n",
    "            data = line.split(',')\n",
    "            key = data[0]\n",
    "            d[key] = []\n",
    "            d1[key] = key\n",
    "            for idx,item in enumerate(data):\n",
    "                if idx != 0 and item != \"x\":\n",
    "                    try:\n",
    "                        pieceNum = int(item)\n",
    "                        d[key].append(pieceNum)\n",
    "                    except:\n",
    "                        pass\n",
    "                    try:\n",
    "                        pieceNum = int(item)\n",
    "                        d1[str(pieceNum)] = key\n",
    "                    except:\n",
    "                        pass\n",
    "    return d, d1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readHypothesisFiles(hypdir, benchmark):\n",
    "    l = []\n",
    "    for hypfile in sorted(glob.glob(\"{}/*.hyp\".format(hypdir))):\n",
    "#         print(hypfile)\n",
    "        with open(hypfile, \"rb\") as f:\n",
    "            l.append(pickle.load(f))\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = set([])\n",
    "def collapseIds(pieceScores):\n",
    "    pieceScores = list(dict.fromkeys(pieceScores))\n",
    "    return pieceScores\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRank(pieceScores,gt,idt,queryid,benchmark, condition):\n",
    "    query = queryid.split('_')[0]\n",
    "    l = []\n",
    "    count = 0\n",
    "    rank = 300000\n",
    "    for i in range(len(pieceScores)):\n",
    "        if pieceScores[i][0][0]=='p':\n",
    "            if(pieceScores[i][0]==queryid.split('_')[0]):\n",
    "                rank=count+1\n",
    "                return rank\n",
    "            if not pieceScores[i][0] in idt.keys():\n",
    "                count+=1\n",
    "            elif not idt[pieceScores[i][0]] in l:\n",
    "                count+=1\n",
    "                l.append(idt[idt[pieceScores[i][0]]])\n",
    "        else:\n",
    "            try:\n",
    "                if (int(pieceScores[i][0].split(\"_\")[-1]) in gt[query]):\n",
    "                    rank = count+1\n",
    "                    return rank\n",
    "\n",
    "                if not str(int(pieceScores[i][0].split(\"_\")[-1])) in idt.keys():\n",
    "                    count+=1\n",
    "                elif not idt[str(int(pieceScores[i][0].split(\"_\")[-1]))] in l:\n",
    "                    count+=1\n",
    "                    l.append(idt[str(int(pieceScores[i][0].split(\"_\")[-1]))])\n",
    "            except:\n",
    "                pass\n",
    "    return rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcPrecisionRecall(hypdir, gtfile, benchmark = False, condition = 1):\n",
    "    \"\"\"Inputs: a directory containing the ranked list of pieces from the marketplace fingerprinting system,\n",
    "               and a file containing the ground truth mapping for each query and the correct IMSLP piece.\n",
    "               \n",
    "       Outputs: the MRR for all the predictions, a list of ranks for each query, a list of runtimes for \n",
    "                each query, and the number of matches processed for each query.\"\"\"\n",
    "    hyps = readHypothesisFiles(hypdir, benchmark)\n",
    "    gt,idt = readGroundTruthLabels(gtfile)\n",
    "    MRR = 0\n",
    "    runtimes = []\n",
    "    MRRs = []\n",
    "    matches = []\n",
    "    hyps = np.array(sorted(hyps, key = lambda x: int(x[0].split(\"_\")[0][1:])*100+int(x[0].split(\"_\")[1][1:])), dtype=object)\n",
    "    count = 0\n",
    "    if hyps.shape[1] == 2:\n",
    "        for queryid, pieceScores in hyps:\n",
    "            rank = getRank(pieceScores,gt,idt,queryid,benchmark, condition)\n",
    "            MRRs.append((queryid,rank))\n",
    "            MRR=MRR+1/(rank)\n",
    "    else:\n",
    "        for queryid, pieceScores, runtime, matches_processed in hyps:\n",
    "            rank = getRank(pieceScores,gt,idt,queryid,benchmark, condition)\n",
    "            MRRs.append((queryid,rank))\n",
    "            runtimes.append(runtime)\n",
    "            MRR=MRR+1/(rank)\n",
    "            matches.append(matches_processed)\n",
    "    return MRR/len(MRRs),MRRs, runtimes, matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcOverlap(seg1, seg2):\n",
    "    overlap_lb = max(seg1[0], seg2[0])\n",
    "    overlap_ub = min(seg1[1], seg2[1])\n",
    "    overlap = np.clip(overlap_ub - overlap_lb, 0, None)\n",
    "    return overlap    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hypdir = 'experiments/v0.4.0d_1k_budget/hyp'\n",
    "benchmark = 0\n",
    "queryGTFile = 'piece_To_id.csv'\n",
    "correct_matches = {}\n",
    "MRR, MRRs, runtimes, matches_processed = calcPrecisionRecall(hypdir, queryGTFile, benchmark, condition = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "MRR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate Errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This prints the individual ranks for each query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printDebuggingInfo(MRRs):\n",
    "    for i, (queryid, rank) in enumerate(MRRs):\n",
    "        print(queryid, rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printDebuggingInfo(MRRs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measure Runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showRuntimeStats(durs):\n",
    "    durs = np.array(durs)\n",
    "    avgDur = np.mean(durs)\n",
    "    minDur = np.min(durs)\n",
    "    maxDur = np.max(durs)\n",
    "    stdDur = np.std(durs)\n",
    "    print('Avg runtime: {:.2f} sec'.format(avgDur))\n",
    "    print('Std runtime: {:.2f} sec'.format(stdDur))\n",
    "    print('Min runtime: {:.2f} sec'.format(minDur))\n",
    "    print('Max runtime: {:.2f} sec'.format(maxDur))\n",
    "    plt.hist(durs, bins=np.arange(0,2,.1))\n",
    "    plt.xlabel('Runtime (sec)')\n",
    "    plt.ylabel('Count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "showRuntimeStats(runtimes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measure matches processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showMatchStats(matchesProcessed):\n",
    "    matches = np.array(matchesProcessed)\n",
    "    avgMatches = np.mean(matches)\n",
    "    minMatches = np.min(matches)\n",
    "    maxMatches = np.max(matches)\n",
    "    stdMatches = np.std(matches)\n",
    "    print(f'Avg matches: {avgMatches}')\n",
    "    print(f'Std matches: {stdMatches}')\n",
    "    print(f'Min matches: {minMatches}')\n",
    "    print(f'Max matches: {maxMatches}')\n",
    "    plt.hist(matches)\n",
    "    plt.xlabel('Number of matches processed')\n",
    "    plt.ylabel('Count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "showMatchStats(matches_processed)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
