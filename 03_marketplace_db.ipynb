{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct database for marketplace fingerprinting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook constructs the database we use during marketplace fingerprinting. It uses the 22 n-gram databases that we previously generated and selects the fingerprint with the highest utility to include in the marketplace database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "import ast\n",
    "import numpy as np\n",
    "from contextlib import closing\n",
    "import numba as nb\n",
    "import pickle\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import multiprocessing\n",
    "import csv\n",
    "import os\n",
    "import time\n",
    "import dill\n",
    "from glob import iglob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in dictionary of probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/data1/kji/databases/probabilities.pkl\", \"rb\") as f:\n",
    "    utilities = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in dictionary of n-grams and their counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combinations = []\n",
    "for n_gram in range(1, 4):\n",
    "    combinations += [[0] + list(tup) for tup in itertools.combinations(range(1, 7), n_gram-1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combinations = [\"\".join(str(num) for num in combination) for combination in combinations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_DB(db, combination, outdir):\n",
    "    with open(f\"{outdir}/{combination}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(db, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test database with in memory dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store each n-gram, utility, and the number of matches for an n-gram in a separate array for memory efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, c in enumerate(combinations):\n",
    "    mapping[c] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_pdfs = 29310"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each n-gram's utility is computed by $P(correctness) \\times m$, where $m$ is the number of times it appears in IMSLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_arrays(fp_matches):\n",
    "    \"\"\"Takes in a fingerprint matches table as input and returns four separate arrays containing the table's \n",
    "       fingerprints, the probability of correctness for each fingerprint, the number of matches for each fingerprint\n",
    "       in IMSLP, and the type of each fingerprint.\"\"\"\n",
    "    n_grams = np.array([], dtype = object)\n",
    "    probabilities = np.array([], dtype = float)\n",
    "    matches = np.array([], dtype = int)\n",
    "    types = np.array([], dtype = int)\n",
    "    # iterate over every n-gram type and add its information to the arrays\n",
    "    for combination in fp_matches:\n",
    "        d = fp_matches[combination]\n",
    "        length = len(d)       \n",
    "        # update indices for the given combination\n",
    "        fps = np.empty(length, dtype = object)\n",
    "        utils = np.empty(length, dtype = float)\n",
    "        values = np.empty(length, dtype = int)\n",
    "        cur_types = np.full(length, mapping[combination])\n",
    "        for i, (n_gram, (count, num_pdfs)) in enumerate(d.items()):\n",
    "            fps[i] = n_gram\n",
    "            values[i] = count\n",
    "            utils[i] = utilities[combination] * count\n",
    "        d.clear()\n",
    "        n_grams = np.concatenate([n_grams, fps])\n",
    "        probabilities = np.concatenate([probabilities, utils])\n",
    "        matches = np.concatenate([matches, values])\n",
    "        types = np.concatenate([types, cur_types])\n",
    "        print(f\"finished processing {combination}\")\n",
    "    return n_grams, probabilities, matches, types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in fingerprint matches table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_dir = '/data1/kji/databases_v4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp_matches_file = f\"{db_dir}/fp_matches.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(fp_matches_file, \"rb\") as f:\n",
    "    fp_matches = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n_grams, probs, matches, types = generate_arrays(fp_matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After generating our four arrays, we store them to use in generating the database construction file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_dir = '/data1/kji/databases_v4_pdfs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_DB(n_grams, \"fps\", db_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_DB(probs, \"utils\", db_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_DB(matches, \"matches\", db_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_DB(types, \"n_gram_types\", db_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate database construction file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of sorting all the n-grams and generating the database in the same step, first we write the sorted list of n-grams to a file for fast iteration speed (instead of regenerating the database every time, we can create multiple files and only generate the database for the finalized file)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combinations = []\n",
    "for n_gram in range(1, 4):\n",
    "    combinations += [[0] + list(tup) for tup in itertools.combinations(range(1, 7), n_gram-1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combinations = [\"\".join(str(num) for num in combination) for combination in combinations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_db_thresholded(n_grams, utilities, types, matches, threshold, outdir, outfile_name):\n",
    "    \"\"\"\n",
    "    Input: an array of n-grams, an array of each n-gram's utility, an array of the number of matches\n",
    "           for each n-gram in IMSLP, and an array of each n-gram's type (e.g. '012')\n",
    "    Output: a file specifying an ordered list of n-grams to include in the final database that have at most threshold matches\n",
    "    \"\"\"\n",
    "    # sort fingerprints in descending order\n",
    "    idx = np.argsort(-utilities)\n",
    "            \n",
    "    # write all used fingerprints to a file\n",
    "    with open(f\"{outdir}/{outfile_name}.txt\", \"w\") as out:\n",
    "        for i in idx:\n",
    "            if matches[i] > threshold:\n",
    "                continue\n",
    "            out.write(f\"{n_grams[i]} {types[i]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_dir = '/data1/kji/databases_v4_pdfs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{db_dir}/fps.pkl\", \"rb\") as f:\n",
    "    n_grams = pickle.load(f)\n",
    "with open(f\"{db_dir}/utils.pkl\", \"rb\") as f:\n",
    "    utilities = pickle.load(f)\n",
    "with open(f\"{db_dir}/n_gram_types.pkl\", \"rb\") as f:\n",
    "    types = pickle.load(f)\n",
    "with open(f\"{db_dir}/matches.pkl\", \"rb\") as f:\n",
    "    matches = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "construct_db_thresholded(n_grams, utilities, types, matches, threshold, \"/data1/kji/construction_lists\", \"all_v4.0.d_pdfs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct database of offsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have selected all the fingerprints, we construct the database containing each fingerprint and their offsets in IMSLP. This file is the output of the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp_file = \"/data1/kji/construction_lists/103mill_v4.0d_pdfs.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct a dictionary with all the n-grams in our database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_mapping = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, c in enumerate(combinations):\n",
    "    reverse_mapping[str(i)] = c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_entry(line):\n",
    "    line = line.rstrip().split()\n",
    "    n_gram, combination = ''.join(line[:-1]), line[-1]\n",
    "    return ast.literal_eval(n_gram), reverse_mapping[combination]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_db(fp_file):\n",
    "    with open(fp_file) as f:\n",
    "        lines = f.readlines()\n",
    "    n_cores = 30\n",
    "    pool = multiprocessing.Pool(n_cores)\n",
    "    keys = pool.map(initialize_entry, lines)\n",
    "    dbs = {combination: {} for combination in combinations}\n",
    "    for fp, combination in keys:\n",
    "        dbs[combination][fp] = {}\n",
    "    return dbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dbs = make_db(fp_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now load in every single n-gram database and update our current database with the real offsets for each n-gram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_dir = '/data1/kji/databases_random'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_db_values(file, db):\n",
    "    with open(file, \"rb\") as f:\n",
    "        d = dill.load(f)\n",
    "    for n_gram in d.keys():\n",
    "        if n_gram in db:\n",
    "            db[n_gram] = dict(d[n_gram])\n",
    "            total_count = 0\n",
    "            for piece in db[n_gram]:\n",
    "                total_count += len(db[n_gram][piece])\n",
    "                db[n_gram][piece] = tuple(db[n_gram][piece])\n",
    "            db[n_gram] = (total_count, db[n_gram])\n",
    "    d.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for combination in dbs:\n",
    "    if dbs[combination]:\n",
    "        add_db_values(f\"{db_dir}/{combination}.pkl\", dbs[combination])\n",
    "        with open(f\"/data1/kji/databases_v4_pdfs/103mill/{combination}.pkl\", \"wb\") as f:\n",
    "            pickle.dump(dbs[combination], f, protocol = pickle.HIGHEST_PROTOCOL)\n",
    "        dbs[combination].clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
